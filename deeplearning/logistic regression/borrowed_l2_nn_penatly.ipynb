{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#some importing\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "#loading data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "#prepare data to have right format for tensorflow\n",
    "#i.e. data is flat matrix, labels are onehot\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "#now is the interesting part - we are building a network with\n",
    "#one hidden ReLU layer and out usual output linear layer\n",
    "\n",
    "#we are going to use SGD so here is our size of batch\n",
    "batch_size = 128\n",
    "\n",
    "#building tensorflow graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #now let's build our new hidden layer\n",
    "  #that's how many hidden neurons we want\n",
    "  num_hidden_neurons = 1024\n",
    "  #its weights\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))\n",
    "\n",
    "  #now the layer itself. It multiplies data by weights, adds biases\n",
    "  #and takes ReLU over result\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "\n",
    "  #time to go for output linear layer\n",
    "  #out weights connect hidden neurons to output labels\n",
    "  #biases are added to output labels  \n",
    "  out_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_neurons, num_labels]))  \n",
    "\n",
    "  out_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "\n",
    "  #compute output  \n",
    "  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases\n",
    "  #our real output is a softmax of prior result\n",
    "  #and we also compute its cross-entropy to get our loss\n",
    "  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_train_labels))\n",
    "\n",
    "  #now we just minimize this loss to actually train the network\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "  #nice, now let's calculate the predictions on each dataset for evaluating the\n",
    "  #performance so far\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(out_layer)\n",
    "  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, out_weights) + out_biases) \n",
    "\n",
    "  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
